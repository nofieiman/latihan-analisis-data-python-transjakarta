{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Latihan Analisis Data Menggunakan Google Colab**\n",
        "\n",
        "Latihan ini merupakan bagian pembelajaran semata untuk melihat implementasi coding/programming menggunakan Python dan Google Colab. Data diunduh dari https://www.kaggle.com/datasets/dikisahkan/transjakarta-transportation-transaction/data?select=dfTransjakarta.csv\n",
        "\n",
        "Versi terakhir 3 November 2025."
      ],
      "metadata": {
        "id": "GX4URuysJAaW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (1) Tahapan Persiapan"
      ],
      "metadata": {
        "id": "cp2hsZAWMUvP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Memuat library yang diperlukan"
      ],
      "metadata": {
        "id": "BEThRViSJT7Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMU7td7pIuoS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import folium"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mengunggah dataset yang akan dipakai"
      ],
      "metadata": {
        "id": "m73puZXvKBYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Option A: direct upload\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "df = pd.read_csv('dfTransjakarta.csv')"
      ],
      "metadata": {
        "id": "6qN-whK9I4mv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bila sudah menggunakan opsi A, opsi ini bisa diabaikan/dilewati."
      ],
      "metadata": {
        "id": "hs4nj4kVKZit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Option B: if stored in Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/dfTransjakarta.csv')"
      ],
      "metadata": {
        "id": "ktpv3IZ8I6Nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Option C: Jika file ada di URL publik\n",
        "\n",
        "url = 'https://example.com/path/to/dfTransjakarta.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "print(f\"Data shape: {df.shape}\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "AglXeyxgVysZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Memeriksa data yang sudah terunggah"
      ],
      "metadata": {
        "id": "RAPOmtGOKfqG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()\n",
        "df.info()\n",
        "df.describe(include='all')"
      ],
      "metadata": {
        "id": "F4TPhHCPI8Hw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Drop missing value"
      ],
      "metadata": {
        "id": "A_kkiN_zLhgV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_droped = df.dropna()\n",
        "df_droped.info()"
      ],
      "metadata": {
        "id": "4Zvn54kMLg-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (2) Exploratory Data Analysis (EDA)"
      ],
      "metadata": {
        "id": "fEvhXHnaLwyL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exploratory Data Analysis (EDA) adalah proses memeriksa dan memahami data secara mendalam dengan menggunakan statistik dan visualisasi. Tujuannya adalah untuk menemukan pola, anomali, hubungan, dan struktur dalam data tanpa membuat asumsi awal. EDA membantu dalam merumuskan hipotesis, memvalidasi asumsi, dan memandu langkah-langkah analisis selanjutnya.\n",
        "\n",
        "Langkah-langkah umum dalam EDA:\n",
        "\n",
        "1.    Memahami Data: Melihat struktur data, jumlah baris dan kolom, tipe data, dll.\n",
        "\n",
        "2.    Pembersihan Data: Menangani missing values, outlier, duplikat, dll.\n",
        "\n",
        "3.    Analisis Statistik Deskriptif: Menghitung mean, median, modus, standar deviasi, dll.\n",
        "\n",
        "4.    Visualisasi Data: Membuat plot seperti histogram, box plot, scatter plot, heatmap, dll.\n",
        "\n",
        "5.    Menemukan Hubungan: Melihat korelasi antara variabel, mengidentifikasi pola, dll.\n",
        "\n",
        "\n",
        "\n",
        "EDA sangat penting karena memberikan wawasan awal yang berharga sebelum membangun model machine learning atau melakukan analisis statistik yang lebih kompleks."
      ],
      "metadata": {
        "id": "iB8wNS5uWDph"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Siapkan dataset yang sudah bersih untuk analisis"
      ],
      "metadata": {
        "id": "a9SgygNAMOP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Kita buat duplikat data frame\n",
        "df1 = df_droped.copy()\n",
        "\n",
        "# View columns\n",
        "df1.columns\n",
        "\n",
        "# Preview dataset\n",
        "df1.head()"
      ],
      "metadata": {
        "id": "Ez7PeHt9L27m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Konversi datetime dalam format waktu"
      ],
      "metadata": {
        "id": "-32aHkBEMrx2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1['tapInTime'] = pd.to_datetime(df1['tapInTime'])\n",
        "df1['tapOutTime'] = pd.to_datetime(df1['tapOutTime'])\n",
        "\n",
        "# Extract time and day features\n",
        "df1['tapInHour'] = df1['tapInTime'].dt.hour\n",
        "df1['tapOutHour'] = df1['tapOutTime'].dt.hour\n",
        "df1['tapDay'] = df1['tapInTime'].dt.dayofweek\n",
        "df1.head()"
      ],
      "metadata": {
        "id": "pJM8oBA5MrUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ubah tapday menjadi hari"
      ],
      "metadata": {
        "id": "onsusbDSMzXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1['tapDay'] = df1['tapDay'].replace({\n",
        "    0: 'Monday', 1: 'Tuesday', 2: 'Wednesday',\n",
        "    3: 'Thursday', 4: 'Friday', 5: 'Saturday', 6: 'Sunday'\n",
        "})\n",
        "df1.head()"
      ],
      "metadata": {
        "id": "4TqLS7doM2Bq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Melihat demografi pengguna"
      ],
      "metadata": {
        "id": "UzxobZ3ZM5Ai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customer = df1[['payCardName', 'payCardBirthDate', 'payCardBank', 'payCardSex']]\n",
        "customer.info()"
      ],
      "metadata": {
        "id": "lZas8yCwM7bK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setiap kali pengguna melakukan transaksi (tap) mewakili satu trip perjalanan, dengan demikian pengguna bisa muncul datanya berulang kali."
      ],
      "metadata": {
        "id": "-fS2AD1QNALb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Menghapus duplikasi data pengguna"
      ],
      "metadata": {
        "id": "--F-FMjnNKjb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customer = customer.drop_duplicates(subset='payCardName', keep='first', ignore_index=True)\n",
        "customer.info()"
      ],
      "metadata": {
        "id": "1_USKOuiNOk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Menghitung usia pengguna"
      ],
      "metadata": {
        "id": "3mMhUFqWNTRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data diperoleh pada 2023\n",
        "customer['age'] = 2023 - customer['payCardBirthDate']\n",
        "customer.head()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ceAi9vdHNSqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Melihat kartu yang digunakan"
      ],
      "metadata": {
        "id": "qkYUSYJDOD8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cust_bank = customer['payCardBank'].value_counts().reset_index(name='count')\n",
        "cust_bank"
      ],
      "metadata": {
        "id": "a9yCR8BwOG8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Menampilkan dalam bar chart."
      ],
      "metadata": {
        "id": "_VlpCceAOJJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ax = cust_bank.plot(x='payCardBank', kind='bar', color='skyblue')\n",
        "\n",
        "# Add text labels\n",
        "for i, v in enumerate(cust_bank['count']):\n",
        "    ax.text(i, v, str(v), ha='center', va='bottom')\n",
        "\n",
        "plt.grid()\n",
        "plt.xlabel('Bank')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Bank Card Distribution')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8OZTptckOK-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Menampilkan dalam pie chart."
      ],
      "metadata": {
        "id": "i895FPUgOM5F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.pie(cust_bank['count'], labels=cust_bank['payCardBank'], autopct='%1.1f%%')\n",
        "plt.axis('equal')\n",
        "plt.title('Bank Card Distribution')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZzEzOKN-OO3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Menambahkan kolom persentase.\n"
      ],
      "metadata": {
        "id": "c9l2seE-ORy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cust_bank['percentage'] = (cust_bank['count'] / cust_bank['count'].sum()) * 100\n",
        "cust_bank"
      ],
      "metadata": {
        "id": "RYn_6UpcOT5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Distribusi usia pelanggan"
      ],
      "metadata": {
        "id": "fCP2GvlmOWEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cust_age = customer[['payCardName', 'age']].groupby('age').size().reset_index(name='count')\n",
        "cust_age"
      ],
      "metadata": {
        "id": "d4GzW35yOYLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Menampilkan dalam line chart."
      ],
      "metadata": {
        "id": "RY56KGVsOafU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ax = cust_age.plot(x='age', kind='line', color='blue')\n",
        "plt.grid()\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Customer Age Distribution')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KM9FT0iHOcGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Distribusi jenis kelamin pelanggan"
      ],
      "metadata": {
        "id": "FZAJnvA8OgcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cust_sex = (\n",
        "    customer.groupby(['payCardSex', 'age'])['payCardName']\n",
        "    .count()\n",
        "    .reset_index(name='count')\n",
        "    .sort_values(by='count', ascending=False)\n",
        ")\n",
        "\n",
        "cust_sex.groupby('payCardSex').size()"
      ],
      "metadata": {
        "id": "XfwrKN3zOiwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualisasikan dalam chart."
      ],
      "metadata": {
        "id": "6-F1EfJPOlF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,6))\n",
        "for sex in cust_sex['payCardSex'].unique():\n",
        "    subset = cust_sex[cust_sex['payCardSex'] == sex]\n",
        "    plt.plot(subset['age'], subset['count'], label=sex)\n",
        "\n",
        "plt.legend()\n",
        "plt.title('Customer Age Distribution by Gender')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Count')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tXr7_0MBOmh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Menunjukkan distribusi jumlah transaksi (tap in/out) per jam."
      ],
      "metadata": {
        "id": "6oYYqK9kUwod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transactions per hour (tap-in)\n",
        "trans_by_hour = df1.groupby('tapInHour').size().reset_index(name='tap_count')\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.bar(trans_by_hour['tapInHour'], trans_by_hour['tap_count'], color='steelblue')\n",
        "plt.xticks(range(0,24))\n",
        "plt.xlabel('Hour of Day')\n",
        "plt.ylabel('Number of Transactions')\n",
        "plt.title('Total Tap-In Transactions by Hour')\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xPkXn_QiUtkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Menunjukkan intensitas perjalanan per hari"
      ],
      "metadata": {
        "id": "AAAtYtkCU1xL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transactions by day of week\n",
        "trans_by_day = df1.groupby('tapDay').size().reindex([\n",
        "    'Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'\n",
        "]).reset_index(name='tap_count')\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.bar(trans_by_day['tapDay'], trans_by_day['tap_count'], color='coral')\n",
        "plt.xlabel('Day of Week')\n",
        "plt.ylabel('Number of Transactions')\n",
        "plt.title('Tap-In Transactions by Day of Week')\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XBVbyV4bU4s6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (3) Recency, Frequency, Monetary (RFM)/Customer Life Value (CLV) Analysis"
      ],
      "metadata": {
        "id": "_S0c9e-XRoCw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recency, Frequency, Monetary (RFM) Analysis adalah metode yang digunakan untuk menganalisis perilaku pelanggan berdasarkan tiga faktor:\n",
        "\n",
        "-    Recency (R): Seberapa baru seorang pelanggan melakukan pembelian? Biasanya diukur dengan jarak hari sejak pembelian terakhir.\n",
        "\n",
        "-    Frequency (F): Seberapa sering seorang pelanggan melakukan pembelian dalam suatu periode? Biasanya dihitung jumlah transaksi.\n",
        "\n",
        "-    Monetary (M): Berapa banyak uang yang dihabiskan oleh pelanggan? Biasanya dijumlahkan total pembelian.\n",
        "\n",
        "Dengan RFM, kita dapat mengelompokkan pelanggan ke dalam segmen-segmen yang berbeda, seperti:\n",
        "\n",
        "-    Pelanggan terbaik (nilai R, F, M tinggi)\n",
        "\n",
        "-    Pelanggan yang hampir hilang (R rendah, F dan M tinggi)\n",
        "\n",
        "-    Pelanggan baru (R tinggi, F dan M rendah)\n",
        "\n",
        "-    Dan lain-lain.\n",
        "\n",
        "Customer Lifetime Value (CLV) Analysis adalah metrik yang mengukur total nilai yang dihasilkan oleh seorang pelanggan selama hubungannya dengan bisnis. CLV membantu perusahaan dalam mengalokasikan sumber daya untuk mempertahankan pelanggan yang bernilai tinggi. Beberapa Pendekatan CLV:\n",
        "\n",
        "-    Historical CLV: Total keuntungan yang dihasilkan pelanggan sejauh ini.\n",
        "\n",
        "-    Predictive CLV: Memprediksi nilai masa depan pelanggan berdasarkan data historis."
      ],
      "metadata": {
        "id": "7SuQY9rtWciw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- RFM / CLV per customer ---\n",
        "# Asumsi: df1 memiliki 'tapOutTime' datetime dan 'payAmount' numeric.\n",
        "# Jika 'tapOutTime' belum datetime, pastikan sebelumnya telah dikonversi.\n",
        "\n",
        "# Referensi tanggal: gunakan tanggal terakhir dalam dataset (maksimum tapOutTime)\n",
        "reference_date = df1['tapOutTime'].max()\n",
        "print(\"Reference date (dataset max tapOutTime):\", reference_date)\n",
        "\n",
        "# Hitung Recency, Frequency, Monetary (CLV proxy)\n",
        "rfm = (df1\n",
        "       .groupby('payCardName')\n",
        "       .agg(\n",
        "           LastTransaction = ('tapOutTime','max'),\n",
        "           Frequency = ('transID','count'),    # jika ada transID unik, gunakan itu; kalau tidak, ganti 'payCardName'\n",
        "           Monetary = ('payAmount','sum')\n",
        "       )\n",
        "       .reset_index()\n",
        ")\n",
        "\n",
        "# Recency dalam hari\n",
        "rfm['Recency'] = (reference_date - rfm['LastTransaction']).dt.days\n",
        "\n",
        "# Tambahkan info demografis (age, bank, sex) dari 'customer' jika ada\n",
        "if 'age' in customer.columns:\n",
        "    rfm = rfm.merge(customer[['payCardName','age','payCardBank','payCardSex']], on='payCardName', how='left')\n",
        "else:\n",
        "    rfm = rfm.merge(customer[['payCardName','payCardBank','payCardSex']], on='payCardName', how='left')\n",
        "\n",
        "# Urutkan untuk tinjauan\n",
        "rfm = rfm.sort_values(by='Monetary', ascending=False).reset_index(drop=True)\n",
        "rfm.head(10)"
      ],
      "metadata": {
        "id": "r0LiEdE-Rp94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Membuat RFM scoring"
      ],
      "metadata": {
        "id": "SIKM7iNZSD-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Scoring RFM (contoh: 5-bucket) ---\n",
        "# Pastikan tidak ada nilai identik untuk pemotongan; gunakan rank() agar stabil\n",
        "rfm['R_score'] = pd.qcut(rfm['Recency'].rank(method='first'), 5, labels=[5,4,3,2,1]).astype(int)   # recency: lebih kecil => lebih baik => label tinggi\n",
        "rfm['F_score'] = pd.qcut(rfm['Frequency'].rank(method='first'), 5, labels=[1,2,3,4,5]).astype(int)\n",
        "rfm['M_score'] = pd.qcut(rfm['Monetary'].rank(method='first'), 5, labels=[1,2,3,4,5]).astype(int)\n",
        "\n",
        "rfm['RFM_Score'] = rfm[['R_score','F_score','M_score']].sum(axis=1)\n",
        "rfm['RFM_Segment'] = rfm['R_score'].astype(str) + rfm['F_score'].astype(str) + rfm['M_score'].astype(str)\n",
        "\n",
        "# contoh: lihat top dan bottom segmen\n",
        "display(rfm.sort_values('RFM_Score', ascending=False).head(10))\n",
        "display(rfm.sort_values('RFM_Score', ascending=True).head(10))"
      ],
      "metadata": {
        "id": "pRXbfARHR_Xb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Memperbaiki durasi perjalanan menggunakan timedelta"
      ],
      "metadata": {
        "id": "2_lnHjWlSJXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Trip duration (hours) akurat menggunakan timedelta ---\n",
        "# Pastikan tapInTime dan tapOutTime sudah datetime\n",
        "df1['tapInTime'] = pd.to_datetime(df1['tapInTime'], errors='coerce')\n",
        "df1['tapOutTime'] = pd.to_datetime(df1['tapOutTime'], errors='coerce')\n",
        "\n",
        "# Hitung durasi (dalam jam, float)\n",
        "df1['tripDuration_hours'] = (df1['tapOutTime'] - df1['tapInTime']).dt.total_seconds() / 3600.0\n",
        "\n",
        "# Periksa nilai negatif (indikasi overnight atau inkonsistensi)\n",
        "neg_count = (df1['tripDuration_hours'] < 0).sum()\n",
        "print(\"Count negative durations:\", neg_count)\n",
        "\n",
        "# Jika ada negative durations, tampilkan contoh untuk inspeksi\n",
        "if neg_count > 0:\n",
        "    display(df1[df1['tripDuration_hours'] < 0].head(10))\n",
        "\n",
        "# Statistik durasi\n",
        "display(df1['tripDuration_hours'].describe().round(3))"
      ],
      "metadata": {
        "id": "7QCSFHC5SM8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rangkuman rute, route counts, top routes"
      ],
      "metadata": {
        "id": "wuIEVSYZSWj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Agregasi rute (tapInStopsName -> tapOutStopsName) ---\n",
        "route_counts = (df1\n",
        "                .groupby(['tapInStopsName','tapOutStopsName'])\n",
        "                .size()\n",
        "                .reset_index(name='TransactionCount')\n",
        "                .sort_values('TransactionCount', ascending=False)\n",
        "                .reset_index(drop=True))\n",
        "\n",
        "route_counts.head(20)"
      ],
      "metadata": {
        "id": "odNhooI1SaBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hitung berapa banyak tap di tiap halte (tap in/out) dan tampilkan top 5"
      ],
      "metadata": {
        "id": "vFjMIR3qSc_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tap in counts\n",
        "tapIn_counts = df1.groupby('tapInStopsName').size().reset_index(name='tapInCounts').sort_values('tapInCounts', ascending=False).reset_index(drop=True)\n",
        "tapIn_counts.head(10)\n",
        "\n",
        "# Tap out counts\n",
        "tapOut_counts = df1.groupby('tapOutStopsName').size().reset_index(name='tapOutCounts').sort_values('tapOutCounts', ascending=False).reset_index(drop=True)\n",
        "tapOut_counts.head(10)"
      ],
      "metadata": {
        "id": "cLULMzznSfT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Kapan waktu terpadat untuk top 5 halte (tap-in dan tap-out)"
      ],
      "metadata": {
        "id": "A6k-90O2Shlu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pastikan kolom jam tersedia\n",
        "df1['tapInHour'] = df1['tapInTime'].dt.hour\n",
        "df1['tapOutHour'] = df1['tapOutTime'].dt.hour\n",
        "\n",
        "# Top 5 tap-in stops\n",
        "top5_in = tapIn_counts.head(5)['tapInStopsName'].tolist()\n",
        "\n",
        "# Busiest hour per stop (tap-in) untuk top5\n",
        "busiest_in = (df1[df1['tapInStopsName'].isin(top5_in)]\n",
        "              .groupby(['tapInStopsName','tapInHour'])\n",
        "              .size()\n",
        "              .reset_index(name='count')\n",
        "              .sort_values(['tapInStopsName','count'], ascending=[True, False])\n",
        "              .drop_duplicates(subset='tapInStopsName', keep='first')\n",
        "              .sort_values('count', ascending=False)\n",
        "             )\n",
        "busiest_in\n",
        "\n",
        "# Untuk tap-out: top5 tap-out stops\n",
        "top5_out = tapOut_counts.head(5)['tapOutStopsName'].tolist()\n",
        "\n",
        "busiest_out = (df1[df1['tapOutStopsName'].isin(top5_out)]\n",
        "               .groupby(['tapOutStopsName','tapOutHour'])\n",
        "               .size()\n",
        "               .reset_index(name='count')\n",
        "               .sort_values(['tapOutStopsName','count'], ascending=[True, False])\n",
        "               .drop_duplicates(subset='tapOutStopsName', keep='first')\n",
        "               .sort_values('count', ascending=False)\n",
        "              )\n",
        "busiest_out"
      ],
      "metadata": {
        "id": "3DmzaCT-SkI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualisasi sederhana (bar chart top 5 tap-in/tap-out)"
      ],
      "metadata": {
        "id": "7i0C12FYSmfF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Top5 tap-in bar\n",
        "top5_in_df = tapIn_counts.head(5).copy()\n",
        "ax = top5_in_df.plot(x='tapInStopsName', y='tapInCounts', kind='bar', legend=False)\n",
        "for i, v in enumerate(top5_in_df['tapInCounts']):\n",
        "    ax.text(i, v, str(v), ha='center', va='bottom')\n",
        "plt.title('Top 5 Tap-In Stops')\n",
        "plt.xlabel('')\n",
        "plt.ylabel('Tap-In Count')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Top5 tap-out bar\n",
        "top5_out_df = tapOut_counts.head(5).copy()\n",
        "ax = top5_out_df.plot(x='tapOutStopsName', y='tapOutCounts', kind='bar', legend=False)\n",
        "for i, v in enumerate(top5_out_df.iloc[:,1]):\n",
        "    ax.text(i, v, str(v), ha='center', va='bottom')\n",
        "plt.title('Top 5 Tap-Out Stops')\n",
        "plt.xlabel('')\n",
        "plt.ylabel('Tap-Out Count')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CdRoHXugSoQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gabungkan tap-in dan tap-out volume per halte"
      ],
      "metadata": {
        "id": "JFZKwlSiVQtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "station_volume = (\n",
        "    pd.concat([\n",
        "        df1[['tapInStopsName']].rename(columns={'tapInStopsName':'station'}),\n",
        "        df1[['tapOutStopsName']].rename(columns={'tapOutStopsName':'station'})\n",
        "    ])\n",
        "    .value_counts()\n",
        "    .reset_index(name='total_transactions')\n",
        "    .rename(columns={'index':'station'})\n",
        ")\n",
        "\n",
        "station_volume_top10 = station_volume.head(10)\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.barh(station_volume_top10['station'], station_volume_top10['total_transactions'], color='teal')\n",
        "plt.xlabel('Total Transactions (Tap-In + Tap-Out)')\n",
        "plt.ylabel('Station')\n",
        "plt.title('Top 10 Busiest Stations')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QvEB8IKYVPBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Peta Folium untuk Top Routes/Top Stops (top N)"
      ],
      "metadata": {
        "id": "0Yn_Z9sdSrqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Siapkan koordinat unik untuk setiap halte (gabungkan in/out coords jika tersedia) ---\n",
        "coords_in = df1[['tapInStopsName','tapInStopsLat','tapInStopsLon']].drop_duplicates(subset='tapInStopsName').rename(columns={\n",
        "    'tapInStopsName':'stopName','tapInStopsLat':'lat','tapInStopsLon':'lon'\n",
        "})\n",
        "coords_out = df1[['tapOutStopsName','tapOutStopsLat','tapOutStopsLon']].drop_duplicates(subset='tapOutStopsName').rename(columns={\n",
        "    'tapOutStopsName':'stopName','tapOutStopsLat':'lat','tapOutStopsLon':'lon'\n",
        "})\n",
        "\n",
        "coords = pd.concat([coords_in, coords_out], ignore_index=True)\n",
        "coords = coords.drop_duplicates(subset='stopName').reset_index(drop=True)\n",
        "\n",
        "# Pilih top N routes (mis. top10 highest transaction count) dan ambil koordinat tapIn\n",
        "top_routes = route_counts.head(10).copy()\n",
        "top_routes = top_routes.merge(coords, left_on='tapInStopsName', right_on='stopName', how='left').rename(columns={'lat':'in_lat','lon':'in_lon'})\n",
        "top_routes = top_routes.merge(coords, left_on='tapOutStopsName', right_on='stopName', how='left').rename(columns={'lat':'out_lat','lon':'out_lon'})\n",
        "\n",
        "# Buat peta terpusat di Jakarta\n",
        "m = folium.Map(location=[-6.1751,106.8272], zoom_start=11)\n",
        "\n",
        "# Tambah marker dan polyline untuk tiap route (jika koordinat tersedia)\n",
        "for _, r in top_routes.iterrows():\n",
        "    if pd.notna(r['in_lat']) and pd.notna(r['in_lon']):\n",
        "        folium.Marker([r['in_lat'], r['in_lon']],\n",
        "                      popup=f\"In: {r['tapInStopsName']}\\nCount: {r['TransactionCount']}\",\n",
        "                      icon=folium.Icon(color='green')).add_to(m)\n",
        "    if pd.notna(r['out_lat']) and pd.notna(r['out_lon']):\n",
        "        folium.Marker([r['out_lat'], r['out_lon']],\n",
        "                      popup=f\"Out: {r['tapOutStopsName']}\\nCount: {r['TransactionCount']}\",\n",
        "                      icon=folium.Icon(color='red')).add_to(m)\n",
        "    # gambar garis rute jika kedua koordinat ada\n",
        "    if pd.notna(r.get('in_lat')) and pd.notna(r.get('out_lat')):\n",
        "        folium.PolyLine(locations=[[r['in_lat'], r['in_lon']],[r['out_lat'], r['out_lon']]],\n",
        "                        weight=2, tooltip=f\"{r['tapInStopsName']} → {r['tapOutStopsName']} ({r['TransactionCount']})\"\n",
        "                       ).add_to(m)\n",
        "\n",
        "# Tampilkan peta (di notebook akan render)\n",
        "m"
      ],
      "metadata": {
        "id": "I-m_KXJqSu6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deteksi outliers durasi (mis. > 6 jam) untuk memeriksa data rusak"
      ],
      "metadata": {
        "id": "tV_VWwpTTSDu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gunakan df1_corrected jika koreksi tadi dijalankan; jika tidak, gunakan df1\n",
        "df_check = df1_corrected if 'df1_corrected' in globals() else df1\n",
        "\n",
        "# pastikan kolom ada\n",
        "if 'tripDuration_hours' not in df_check.columns:\n",
        "    df_check['tripDuration_hours'] = (df_check['tapOutTime'] - df_check['tapInTime']).dt.total_seconds() / 3600.0\n",
        "\n",
        "# threshold outlier (adjustable)\n",
        "threshold_hours = 6.0\n",
        "outliers = df_check[df_check['tripDuration_hours'] > threshold_hours].copy()\n",
        "print(\"Jumlah trip dengan durasi >\", threshold_hours, \"jam:\", len(outliers))\n",
        "print(\"Persentase dari total:\", round(100*len(outliers)/len(df_check),4), \"%\")\n",
        "display(outliers[['payCardName','tapInStopsName','tapOutStopsName','tapInTime','tapOutTime','tripDuration_hours']].head(20))"
      ],
      "metadata": {
        "id": "7CPxHGISTUvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Heatmap jam × halte (matrix) untuk melihat pola puncak harian"
      ],
      "metadata": {
        "id": "J67q51l_TC-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_heat = df_check.copy()\n",
        "df_heat['tapInHour'] = df_heat['tapInTime'].dt.hour\n",
        "\n",
        "# pilih top N stops berdasarkan total tap-in\n",
        "top_n = 20\n",
        "top_stops = df_heat['tapInStopsName'].value_counts().head(top_n).index.tolist()\n",
        "\n",
        "# pivot table: index = stop, columns = hour, values = counts\n",
        "pivot = (df_heat[df_heat['tapInStopsName'].isin(top_stops)]\n",
        "         .groupby(['tapInStopsName','tapInHour'])\n",
        "         .size()\n",
        "         .reset_index(name='count')\n",
        "        )\n",
        "\n",
        "pivot_table = pivot.pivot(index='tapInStopsName', columns='tapInHour', values='count').fillna(0)\n",
        "\n",
        "# sort stops by total volume for nicer plotting\n",
        "pivot_table['total'] = pivot_table.sum(axis=1)\n",
        "pivot_table = pivot_table.sort_values('total', ascending=False).drop(columns='total')\n",
        "\n",
        "# plot heatmap\n",
        "plt.figure(figsize=(14,10))\n",
        "plt.imshow(pivot_table, aspect='auto', interpolation='nearest')\n",
        "plt.colorbar(label='Tap-In Count')\n",
        "plt.yticks(range(len(pivot_table.index)), pivot_table.index)\n",
        "plt.xticks(range(0,24), range(0,24))\n",
        "plt.xlabel('Hour of Day')\n",
        "plt.title(f'Heatmap: Tap-In Counts per Hour for Top {top_n} Stops')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "urzKliHATGrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Segmentasi RFM lebih lanjut (k-means clustering) pada fitur RFM (setelah scaling)"
      ],
      "metadata": {
        "id": "CCi55_VeTefq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import numpy as np\n",
        "\n",
        "# gunakan rfm yang sudah ada; pastikan kolom Recency, Frequency, Monetary ada\n",
        "rfm_model = rfm[['payCardName','Recency','Frequency','Monetary']].copy()\n",
        "\n",
        "# log-transform Monetary & Frequency to reduce skew (add small const to avoid log(0))\n",
        "rfm_model['log_M'] = np.log1p(rfm_model['Monetary'])\n",
        "rfm_model['log_F'] = np.log1p(rfm_model['Frequency'])\n",
        "rfm_model['R'] = rfm_model['Recency']  # keep as-is or transform if skewed\n",
        "\n",
        "X = rfm_model[['R','log_F','log_M']].values\n",
        "\n",
        "# scale\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Elbow method + silhouette for k in range\n",
        "sse = []\n",
        "sil_scores = []\n",
        "K_range = range(2,9)\n",
        "for k in K_range:\n",
        "    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    km.fit(X_scaled)\n",
        "    sse.append(km.inertia_)\n",
        "    sil_scores.append(silhouette_score(X_scaled, km.labels_))\n",
        "\n",
        "# show results\n",
        "print(\"K\\tSSE\\tSilhouette\")\n",
        "for k,s,e in zip(K_range,sse,sil_scores):\n",
        "    print(k, round(s,2), round(e,3))\n",
        "\n",
        "# choose k (example: pick k with good silhouette and elbow), here use k=4 as example\n",
        "k_best = 4\n",
        "km = KMeans(n_clusters=k_best, random_state=42, n_init=20)\n",
        "labels = km.fit_predict(X_scaled)\n",
        "\n",
        "rfm_model['cluster'] = labels\n",
        "rfm_model['cluster'].value_counts()\n",
        "\n",
        "# Centroids in original scaled space -> transform back for interpretation\n",
        "centroids_scaled = km.cluster_centers_\n",
        "centroids_unscaled = scaler.inverse_transform(centroids_scaled)\n",
        "centroids = pd.DataFrame(centroids_unscaled, columns=['R','log_F','log_M'])\n",
        "centroids['Frequency'] = np.expm1(centroids['log_F']).round(2)\n",
        "centroids['Monetary'] = np.expm1(centroids['log_M']).round(2)\n",
        "centroids[['R','Frequency','Monetary']]\n",
        "\n",
        "# Cluster dengan low Recency, high Frequency, high Monetary = best customers.\n",
        "# Cluster dengan high Recency, low Frequency, low Monetary = churned/low-value.\n",
        "# Gunakan centroid values untuk memberi label (Champions, At-risk, Loyal, etc.)."
      ],
      "metadata": {
        "id": "bdkz6hnYTiVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install lifetimes if belum ada (jalankan sekali di Colab)\n",
        "!pip install lifetimes\n",
        "\n",
        "from lifetimes import BetaGeoFitter, GammaGammaFitter\n",
        "from lifetimes.utils import summary_data_from_transaction_data\n",
        "\n",
        "# 1) persiapkan data transaksi: pastikan df1 punya kolom customer id dan transaction datetime & monetary\n",
        "transactions = df1[['payCardName','tapOutTime','payAmount']].copy().rename(columns={\n",
        "    'payCardName':'customer_id','tapOutTime':'date','payAmount':'monetary_value'\n",
        "})\n",
        "transactions = transactions.dropna(subset=['date'])  # ensure dates exist\n",
        "\n",
        "# 2) summary table needed by lifetimes: uses weekly/days as frequency unit (use days)\n",
        "summary = summary_data_from_transaction_data(transactions, 'customer_id', 'date', monetary_value_col='monetary_value', observation_period_end=transactions['date'].max())\n",
        "\n",
        "# summary has: frequency, recency (in days), T (in days), monetary_avg\n",
        "summary.head()\n",
        "\n",
        "# 3) Fit BG/NBD model (for repeat transaction prediction)\n",
        "bgf = BetaGeoFitter(penalizer_coef=0.0)\n",
        "bgf.fit(summary['frequency'], summary['recency'], summary['T'])\n",
        "print(bgf.summary)\n",
        "\n",
        "# 4) Fit Gamma-Gamma for monetary\n",
        "ggf = GammaGammaFitter(penalizer_coef=0.0)\n",
        "# lifetimes requires customers with frequency > 0 and monetary_avg > 0\n",
        "summary_gg = summary[summary['monetary_value'] > 0].copy() # Use monetary_value\n",
        "ggf.fit(summary_gg['frequency'], summary_gg['monetary_value']) # Use monetary_value\n",
        "print(ggf.summary)\n",
        "\n",
        "# 5) Predict CLTV (expected monetary value * expected repeat transactions) for horizon (e.g., 30 days)\n",
        "expected_purchases_30 = bgf.conditional_expected_number_of_purchases_up_to_time(30, summary['frequency'], summary['recency'], summary['T'])\n",
        "expected_avg_value = ggf.conditional_expected_average_profit(summary['frequency'], summary['monetary_value']) # Use monetary_value\n",
        "clv_30 = expected_purchases_30 * expected_avg_value\n",
        "summary['clv_30_days'] = clv_30\n",
        "display(summary[['frequency','recency','T','monetary_value','clv_30_days']].sort_values('clv_30_days', ascending=False).head(10)) # Use monetary_value"
      ],
      "metadata": {
        "id": "I5NvdKxoT3Un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (4) Pipeline Modeling"
      ],
      "metadata": {
        "id": "ncIjwZqxXJdX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pipeline adalah cara untuk mengotomatiskan dan mengorganisir alur kerja (workflow) machine learning secara berurutan. Bayangkan seperti jalur perakitan di pabrik dimana data melewati berbagai tahap proses secara sistematis hingga akhirnya menghasilkan model.\n",
        "\n",
        "Pipeline biasanya terdiri dari beberapa tahap:\n",
        "\n",
        "1. Preprocessing & Feature Engineering\n",
        "\n",
        "- Scaling/Normalization\n",
        "- Handling missing values\n",
        "- Encoding categorical variables\n",
        "- Feature selection\n",
        "- Dimensionality reduction (PCA)\n",
        "\n",
        "2. Model Training\n",
        "\n",
        "- Memilih algoritma (Random Forest, SVM, dll)\n",
        "- Training model\n",
        "- Hyperparameter tuning\n",
        "\n",
        "3. Evaluation & Deployment\n",
        "- Validasi model\n",
        "- Testing\n",
        "- Deployment ke production"
      ],
      "metadata": {
        "id": "TwAsh8-LVarX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Safety checks & prepare CLV/RFM table ---\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# 1) Ensure datetime for safety (no import/cleaning repeated if already done)\n",
        "df1['tapOutTime'] = pd.to_datetime(df1['tapOutTime'], errors='coerce')\n",
        "\n",
        "# 2) reference date = last observed tapOutTime\n",
        "reference_date = df1['tapOutTime'].max()\n",
        "print(\"Reference date:\", reference_date)\n",
        "\n",
        "# 3) Frequency: prefer unique transID if exists, otherwise count rows\n",
        "if 'transID' in df1.columns:\n",
        "    freq = df1.groupby('payCardName')['transID'].nunique().reset_index(name='Frequency')\n",
        "else:\n",
        "    freq = df1.groupby('payCardName').size().reset_index(name='Frequency')\n",
        "\n",
        "# 4) Monetary (Value): sum of payAmount (safe coercion to numeric)\n",
        "df1['payAmount'] = pd.to_numeric(df1['payAmount'], errors='coerce').fillna(0)\n",
        "monetary = df1.groupby('payCardName')['payAmount'].sum().reset_index(name='Value')\n",
        "\n",
        "# 5) Last transaction date -> Recency (days since last transaction)\n",
        "last_tx = df1.groupby('payCardName')['tapOutTime'].max().reset_index(name='LastTransaction')\n",
        "last_tx['Recency'] = (reference_date - last_tx['LastTransaction']).dt.days\n",
        "\n",
        "# 6) Merge into one CLV/RFM table\n",
        "clv_simple = freq.merge(last_tx[['payCardName','Recency']], on='payCardName').merge(monetary, on='payCardName')\n",
        "\n",
        "# 7) Final columns & quick check\n",
        "clv_simple = clv_simple[['payCardName','Recency','Frequency','Value']].reset_index(drop=True)\n",
        "clv_simple.head()\n",
        "\n",
        "# 8) (Optional) scale for clustering (MinMaxScaler)\n",
        "scaler = MinMaxScaler()\n",
        "cols_to_scale = ['Recency','Frequency','Value']\n",
        "clv_scaled = clv_simple.copy()\n",
        "clv_scaled[cols_to_scale] = scaler.fit_transform(clv_scaled[cols_to_scale])\n",
        "\n",
        "# show scaled sample\n",
        "clv_scaled.head()"
      ],
      "metadata": {
        "id": "2ioSAOpnXKlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tentukan range cluster dan evaluasi dengan Elbow & Silhouette"
      ],
      "metadata": {
        "id": "bPLUaXzBX0DA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Elbow Method adalah teknik yang digunakan untuk membantu menentukan jumlah kluster (K) yang optimal dalam algoritma K-Means. Metode ini didasarkan pada prinsip bahwa kita ingin memilih K yang meminimalkan variansi dalam kluster (dihitung dengan Within-Cluster Sum of Squares atau WCSS) tetapi tanpa menambahkan terlalu banyak kompleksitas (yaitu, tidak memilih K yang terlalu besar).\n",
        "\n",
        "Disebut \"elbow\" karena plotnya sering menyerupai lengan, di mana terdapat penurunan WCSS yang cepat pada awalnya (seperti dari K=1 ke K=2, dan seterusnya) dan kemudian penurunan menjadi lebih landai setelah suatu titik. Titik di mana perubahan dari cepat ke landai itulah yang disebut \"siku\".\n",
        "\n",
        "Silhouette Analysis adalah metode lain untuk mengevaluasi kualitas kluster dan menentukan K yang optimal. Metode ini mengukur seberapa baik setiap titik data dalam satu kluster cocok dengan klusternya sendiri dibandingkan dengan kluster terdekat lainnya.\n",
        "\n",
        "Silhouette Analysis dapat memberikan ukuran yang lebih interpretatif tentang kualitas kluster. Ia juga dapat membantu membandingkan berbagai ukuran K dan juga berbagai algoritma kluster.\n",
        "\n",
        "Kedua metode ini saling melengkapi. Dalam praktiknya, sering kali kita menggunakan kedua metode tersebut dan mempertimbangkan hasil keduanya untuk memilih K yang optimal. Selain itu, pertimbangan domain knowledge juga sangat penting."
      ],
      "metadata": {
        "id": "garavGOUUt9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Evaluasi jumlah cluster (Elbow & Silhouette) ---\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "X = clv_scaled[['Recency', 'Frequency', 'Value']]\n",
        "\n",
        "ssd = []                # Sum of Squared Distances\n",
        "silhouette_scores = []  # Silhouette Coefficient\n",
        "\n",
        "range_n_clusters = range(2, 9)\n",
        "\n",
        "for k in range_n_clusters:\n",
        "    km = KMeans(n_clusters=k, random_state=42, n_init=20)\n",
        "    km.fit(X)\n",
        "    ssd.append(km.inertia_)\n",
        "    silhouette_scores.append(silhouette_score(X, km.labels_))\n",
        "\n",
        "# Plot hasil evaluasi\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(range_n_clusters, ssd, marker='o')\n",
        "plt.xlabel('Jumlah Cluster (k)')\n",
        "plt.ylabel('SSD (Inertia)')\n",
        "plt.title('Elbow Curve')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(range_n_clusters, silhouette_scores, marker='o', color='orange')\n",
        "plt.xlabel('Jumlah Cluster (k)')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.title('Silhouette Analysis')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "for k, s in zip(range_n_clusters, silhouette_scores):\n",
        "    print(f\"k={k} → Silhouette Score: {s:.3f}\")"
      ],
      "metadata": {
        "id": "nRWiFFy4X08K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fit model K-Means dengan k terbaik"
      ],
      "metadata": {
        "id": "FqL452koX4v6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-Means adalah sebuah algoritma unsupervised learning (pembelajaran tanpa pengawasan) yang digunakan untuk pengelompokan data (clustering). Tujuannya adalah untuk mengelompokkan data menjadi K buah kluster (kelompok), di mana setiap data dalam satu kluster memiliki karakteristik yang serupa, dan data dari kluster yang berbeda memiliki karakteristik yang berbeda.\n",
        "\n",
        "Huruf \"K\" mewakili jumlah kluster yang ingin dibentuk, dan harus ditentukan oleh kita sebelum algoritma dijalankan."
      ],
      "metadata": {
        "id": "gZkIQh4tUhV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Fit KMeans final ---\n",
        "k_best = 4  # ubah sesuai hasil Elbow & Silhouette\n",
        "km_final = KMeans(n_clusters=k_best, random_state=42, n_init=30)\n",
        "clv_scaled['Cluster'] = km_final.fit_predict(X)\n",
        "\n",
        "# Simpan label ke versi asli\n",
        "clv_simple['Cluster'] = clv_scaled['Cluster']\n",
        "\n",
        "# Lihat distribusi cluster\n",
        "print(\"Distribusi jumlah pelanggan per cluster:\\n\")\n",
        "print(clv_simple['Cluster'].value_counts().sort_index())"
      ],
      "metadata": {
        "id": "qCqo9dIEX5Xw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualisasi hasil (2D PCA & 3D scatter)"
      ],
      "metadata": {
        "id": "VPPiFXBoX8Lr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# --- PCA 2D projection untuk visualisasi ---\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "pca_coords = pca.fit_transform(X)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(pca_coords[:,0], pca_coords[:,1],\n",
        "            c=clv_scaled['Cluster'], cmap='tab10', s=40, alpha=0.7)\n",
        "plt.xlabel('PCA1')\n",
        "plt.ylabel('PCA2')\n",
        "plt.title('Visualisasi Cluster (PCA 2D)')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# --- 3D scatter (opsional) ---\n",
        "fig = plt.figure(figsize=(8,6))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(clv_scaled['Recency'], clv_scaled['Frequency'], clv_scaled['Value'],\n",
        "           c=clv_scaled['Cluster'], cmap='tab10', s=20)\n",
        "ax.set_xlabel('Recency (scaled)')\n",
        "ax.set_ylabel('Frequency (scaled)')\n",
        "ax.set_zlabel('Value (scaled)')\n",
        "ax.set_title('3D View: Cluster Pelanggan (R, F, V)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "G_976HLLX-I8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Profiling & interpretasi cluster"
      ],
      "metadata": {
        "id": "Z2xfZaTCYAf3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Profiling centroid & statistik per cluster ---\n",
        "centroids_scaled = km_final.cluster_centers_\n",
        "centroids_unscaled = pd.DataFrame(\n",
        "    scaler.inverse_transform(centroids_scaled),\n",
        "    columns=['Recency','Frequency','Value']\n",
        ")\n",
        "centroids_unscaled['Cluster'] = range(k_best)\n",
        "centroids_unscaled = centroids_unscaled[['Cluster','Recency','Frequency','Value']]\n",
        "\n",
        "print(\"Centroid tiap cluster (dalam skala asli):\")\n",
        "display(centroids_unscaled.round(2))\n",
        "\n",
        "# Statistik ringkas per cluster\n",
        "profile = (clv_simple.groupby('Cluster')\n",
        "            .agg(Count=('payCardName','count'),\n",
        "                 Recency_mean=('Recency','mean'),\n",
        "                 Frequency_mean=('Frequency','mean'),\n",
        "                 Value_mean=('Value','mean'))\n",
        "            .sort_index())\n",
        "display(profile.round(2))"
      ],
      "metadata": {
        "id": "yiKKlFgAYB3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Boxplot distribusi per fitur"
      ],
      "metadata": {
        "id": "Ococ49PAYEJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,4))\n",
        "plt.subplot(1,3,1)\n",
        "clv_simple.boxplot(column='Recency', by='Cluster')\n",
        "plt.title('Recency per Cluster'); plt.suptitle('')\n",
        "\n",
        "plt.subplot(1,3,2)\n",
        "clv_simple.boxplot(column='Frequency', by='Cluster')\n",
        "plt.title('Frequency per Cluster')\n",
        "\n",
        "plt.subplot(1,3,3)\n",
        "clv_simple.boxplot(column='Value', by='Cluster')\n",
        "plt.title('Value per Cluster')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hBHazbF3YaIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analisis naratif otomatis per cluster"
      ],
      "metadata": {
        "id": "i2kPU8YjYtqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalisasi agar dapat dibandingkan relatif antar-cluster\n",
        "desc_stats = centroids_unscaled.copy()\n",
        "desc_stats_norm = desc_stats[['Recency','Frequency','Value']].apply(\n",
        "    lambda x: (x - x.min()) / (x.max() - x.min())\n",
        ")\n",
        "desc_stats_norm['Cluster'] = desc_stats['Cluster']\n",
        "\n",
        "cluster_narratives = []\n",
        "\n",
        "for _, row in desc_stats_norm.iterrows():\n",
        "    cluster_id = int(row['Cluster'])\n",
        "    rec, freq, val = row['Recency'], row['Frequency'], row['Value']\n",
        "\n",
        "    # --- aturan deskriptif sederhana ---\n",
        "    if rec < 0.3 and freq > 0.7 and val > 0.7:\n",
        "        label = \"Pelanggan sangat aktif dan bernilai tinggi (Champions)\"\n",
        "        mood = \"🟢\"\n",
        "    elif rec < 0.5 and freq > 0.5:\n",
        "        label = \"Pelanggan aktif dengan nilai sedang (Regular Loyal)\"\n",
        "        mood = \"🟡\"\n",
        "    elif rec > 0.7 and freq < 0.4:\n",
        "        label = \"Pelanggan dorman atau berisiko hilang (At Risk)\"\n",
        "        mood = \"🔴\"\n",
        "    else:\n",
        "        label = \"Pelanggan sporadis / nilai rendah (Casual)\"\n",
        "        mood = \"⚪\"\n",
        "\n",
        "    narrative = (f\"Cluster {cluster_id}: {mood} \"\n",
        "                 f\"Recency={rec:.2f}, Frequency={freq:.2f}, Value={val:.2f} → {label}.\")\n",
        "    cluster_narratives.append(narrative)\n",
        "\n",
        "print(\"\\n=== Narasi Otomatis per Cluster ===\\n\")\n",
        "for n in cluster_narratives:\n",
        "    print(n)"
      ],
      "metadata": {
        "id": "ahMYQesCYuTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (5) Forecasting Transaksi/Retensi (Probabilistik Model)"
      ],
      "metadata": {
        "id": "S0MLGoBgdF0J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pendekatan ini cocok untuk data seperti TransJakarta karena bersifat event-based (tiap tap = transaksi). Model paling umum: BG/NBD + Gamma-Gamma. Tujuannya memprediksi berapa kali pelanggan akan bertransaksi lagi (frequency prediction), dan memperkirakan nilai moneter masa depan (Customer Lifetime Value)."
      ],
      "metadata": {
        "id": "aLc5Bu0EdMNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install lifetimes if belum ada (jalankan sekali)\n",
        "!pip install lifetimes --quiet\n",
        "\n",
        "from lifetimes import BetaGeoFitter, GammaGammaFitter\n",
        "from lifetimes.utils import summary_data_from_transaction_data\n",
        "from lifetimes.plotting import plot_frequency_recency_matrix, plot_period_transactions\n",
        "import pandas as pd\n",
        "\n",
        "# --- Defensive: pastikan df1 ada dan kolom yang diperlukan tersedia ---\n",
        "# Required columns: 'payCardName' (customer id), 'tapOutTime' (datetime), 'payAmount' (monetary)\n",
        "# If tapOutTime missing for some rows, consider using tapInTime instead (but be consistent).\n",
        "\n",
        "# 1) Prepare transactions dataframe\n",
        "transactions = df1[['payCardName','tapOutTime','payAmount']].copy().rename(columns={\n",
        "    'payCardName':'customer_id','tapOutTime':'date','payAmount':'monetary_value'\n",
        "})\n",
        "transactions = transactions.dropna(subset=['customer_id','date'])  # require id + date\n",
        "transactions['date'] = pd.to_datetime(transactions['date'], errors='coerce')\n",
        "transactions = transactions.dropna(subset=['date'])  # drop rows with invalid dates\n",
        "\n",
        "# Ensure monetary numeric and non-negative\n",
        "transactions['monetary_value'] = pd.to_numeric(transactions['monetary_value'], errors='coerce').fillna(0)\n",
        "transactions.loc[transactions['monetary_value'] < 0, 'monetary_value'] = 0\n",
        "\n",
        "# 2) Build summary table required by lifetimes\n",
        "# observation_period_end optional: use max date in transactions\n",
        "observation_end = transactions['date'].max()\n",
        "print(\"Observation period end:\", observation_end)\n",
        "\n",
        "summary = summary_data_from_transaction_data(\n",
        "    transactions,\n",
        "    customer_id_col='customer_id',\n",
        "    datetime_col='date',\n",
        "    monetary_value_col='monetary_value',\n",
        "    observation_period_end=observation_end\n",
        ")\n",
        "# summary columns: frequency, recency, T, monetary_avg\n",
        "# summary = summary.reset_index().rename(columns={'index':'CustomerID'}).set_index('CustomerID') # This renaming caused previous error, remove\n",
        "\n",
        "# Quick checks\n",
        "print(\"Summary shape:\", summary.shape)\n",
        "display(summary.head())\n",
        "\n",
        "# 3) Filter customers usable for modeling\n",
        "# BG/NBD requires frequency >= 0 (frequency = number of repeat purchases)\n",
        "# Gamma-Gamma requires monetary_avg > 0\n",
        "summary_for_bgnbd = summary.copy()   # BG/NBD can work with freq >=0 (but freq=0 means no repeat purchases)\n",
        "summary_for_gg = summary[summary['monetary_value'] > 0].copy() # Use monetary_value here\n",
        "\n",
        "# 4) Fit BG/NBD (Beta-Geometric / Negative Binomial)\n",
        "bgf = BetaGeoFitter(penalizer_coef=0.001)\n",
        "# Note: arguments are (frequency, recency, T)\n",
        "bgf.fit(summary_for_bgnbd['frequency'], summary_for_bgnbd['recency'], summary_for_bgnbd['T'])\n",
        "print(\"BG/NBD fitted. params:\", bgf.params_)\n",
        "\n",
        "# Optional: visualize frequency-recency matrix (diagnostic)\n",
        "# plot_frequency_recency_matrix(bgf) # Move plotting to a separate cell\n",
        "# plot_period_transactions(bgf) # Move plotting to a separate cell\n",
        "\n",
        "# 5) Fit Gamma-Gamma model (monetary), requires customers with monetary_avg > 0 and frequency > 0\n",
        "ggf = GammaGammaFitter(penalizer_coef=0.001)\n",
        "# lifetimes requires customers with frequency > 0 and monetary_avg > 0\n",
        "# Use monetary_value for the monetary data in Gamma-Gamma fitting\n",
        "ggf.fit(summary_for_gg['frequency'], summary_for_gg['monetary_value'])\n",
        "print(\"Gamma-Gamma fitted. params:\", ggf.params_)\n",
        "\n",
        "# 6) Make predictions\n",
        "# Predict expected purchases in next 30 days\n",
        "summary['predicted_purchases_30d'] = bgf.conditional_expected_number_of_purchases_up_to_time(\n",
        "    30,\n",
        "    summary['frequency'],\n",
        "    summary['recency'],\n",
        "    summary['T']\n",
        ")\n",
        "\n",
        "# Predict expected average transaction value (for customers with monetary_avg > 0)\n",
        "# For those with monetary_value == 0, we can leave NaN or impute.\n",
        "summary.loc[summary['monetary_value'] > 0, 'expected_avg_value'] = ggf.conditional_expected_average_profit(\n",
        "    summary.loc[summary['monetary_value'] > 0, 'frequency'],\n",
        "    summary.loc[summary['monetary_value'] > 0, 'monetary_value']\n",
        ")\n",
        "\n",
        "# 7) Predict CLV: use lifetimes' customer_lifetime_value (returns expected total monetary value)\n",
        "# Here we compute 3-month CLV (time=3 months), freq='D' because our recency/T are in days\n",
        "# Need to ensure frequency and monetary_value are passed correctly\n",
        "summary['predicted_clv_3m'] = ggf.customer_lifetime_value(\n",
        "    bgf,\n",
        "    summary['frequency'],\n",
        "    summary['recency'],\n",
        "    summary['T'], # Pass T as well\n",
        "    summary['monetary_value'].fillna(0), # Pass monetary value, handle NaNs\n",
        "    time=3*30,       # time in days (3 months * 30 days/month)\n",
        "    freq='D',     # frequency of the data: 'D' for days\n",
        "    discount_rate=0.01\n",
        ")\n",
        "\n",
        "\n",
        "# 8) Inspect top customers by predicted CLV\n",
        "top_clv = summary.sort_values('predicted_clv_3m', ascending=False).head(10)\n",
        "display(top_clv[['frequency','recency','T','monetary_value','predicted_purchases_30d','predicted_clv_3m']])\n",
        "\n",
        "# 9) Merge predictions back to original CLV table (optional)\n",
        "# Assuming clv_simple has 'payCardName' as a regular column\n",
        "clv_simple_reset = clv_simple.reset_index() # Reset index to merge on 'payCardName'\n",
        "summary_reset = summary.reset_index().rename(columns={'customer_id': 'payCardName'}) # Reset index and rename for merge\n",
        "\n",
        "clv_with_preds = clv_simple_reset.merge(\n",
        "    summary_reset[['payCardName', 'predicted_purchases_30d', 'predicted_clv_3m']],\n",
        "    on='payCardName',\n",
        "    how='left'\n",
        ")\n",
        "display(clv_with_preds[['payCardName','Recency','Frequency','Value','predicted_purchases_30d','predicted_clv_3m']].head(10))"
      ],
      "metadata": {
        "id": "LjWtvhwcdUzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from lifetimes.plotting import plot_frequency_recency_matrix, plot_period_transactions\n",
        "\n",
        "# Pastikan summary ada dan memiliki kolom 'T'\n",
        "print(\"Max T (observation period in days):\", summary['T'].max())\n",
        "\n",
        "# 1) Frequency-Recency matrix — T harus skalar (mis. maksimum T dari summary)\n",
        "T_scalar = summary['T'].max()\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plot_frequency_recency_matrix(bgf, T_scalar)   # bgf + scalar T\n",
        "plt.title('Frequency–Recency Matrix (BG/NBD)')\n",
        "plt.show()\n",
        "\n",
        "# 2) Period transactions plot — visualisasi model fit vs actual\n",
        "# plot_period_transactions can be called with the model alone\n",
        "plt.figure(figsize=(8,6))\n",
        "plot_period_transactions(bgf)\n",
        "plt.title('Model Fit vs Actual Transactions (BG/NBD)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "chz89ntVdWvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Forecasting Volume Lalu Lintas (Time Series) dengan ARIMA/SARIMA"
      ],
      "metadata": {
        "id": "si5ptE9qdaJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Agregasi transaksi mingguan\n",
        "ts = df1.set_index('tapInTime').resample('W')['transID'].count()\n",
        "ts = ts.fillna(0)\n",
        "\n",
        "# Split data train-test (e.g., last 4 weeks for testing)\n",
        "test_size = 4\n",
        "train = ts[:-test_size]\n",
        "test = ts[-test_size:]\n",
        "\n",
        "print(f\"Length of train data: {len(train)}\")\n",
        "print(f\"Length of test data: {len(test)}\")\n",
        "\n",
        "# Fit SARIMA model\n",
        "# Choose appropriate order and seasonal order based on EDA (example values)\n",
        "# (p,d,q) order and (P,D,Q,S) seasonal order\n",
        "# Seasonal period S=52 for weekly data if a year of data is present\n",
        "# For this dataset, S=4 might be more appropriate given the data span (monthly)\n",
        "try:\n",
        "    model = SARIMAX(train, order=(1, 1, 1), seasonal_order=(1, 1, 1, 4))\n",
        "    result = model.fit(disp=False)\n",
        "\n",
        "    # Forecast test_size steps ahead\n",
        "    forecast = result.get_forecast(steps=test_size)\n",
        "    forecast_ci = forecast.conf_int()\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(train, label='Train')\n",
        "    plt.plot(test, label='Test')\n",
        "    plt.plot(forecast.predicted_mean, label='Forecast', color='red')\n",
        "    plt.fill_between(forecast_ci.index, forecast_ci.iloc[:, 0], forecast_ci.iloc[:, 1], color='pink', alpha=0.3)\n",
        "    plt.legend()\n",
        "    plt.title('Forecasting Volume Penumpang Mingguan')\n",
        "    plt.show()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during SARIMA fitting or forecasting: {e}\")\n",
        "    print(\"Consider adjusting the SARIMA order or seasonal order if the error persists.\")"
      ],
      "metadata": {
        "id": "J43AEOFKdbHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prediksi Churn/Aktivitas Pelanggan (Machine Learning)"
      ],
      "metadata": {
        "id": "Ix8eFJfhdew_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Ensure clv_simple is available and has the necessary columns\n",
        "if 'clv_simple' not in globals():\n",
        "    print(\"Error: 'clv_simple' DataFrame not found. Please run the previous cells to generate it.\")\n",
        "else:\n",
        "    # Merge age from customer DataFrame\n",
        "    # Assuming 'customer' DataFrame with 'payCardName' and 'age' exists from previous steps\n",
        "    if 'customer' in globals() and 'age' in customer.columns:\n",
        "        clv_demo = clv_simple.merge(customer[['payCardName','age']], on='payCardName', how='left')\n",
        "    else:\n",
        "         print(\"Warning: 'customer' DataFrame or 'age' column not found. Proceeding without age.\")\n",
        "         clv_demo = clv_simple.copy()\n",
        "\n",
        "\n",
        "    # Define features (X) and target (y)\n",
        "    # Check if 'age' was successfully merged before including it\n",
        "    features = ['Recency','Frequency','Value']\n",
        "    if 'age' in clv_demo.columns:\n",
        "        features.append('age')\n",
        "\n",
        "    X = clv_demo[features]\n",
        "\n",
        "    # Example definition of churn proxy: Customers who have not made a transaction recently (e.g., Recency > median Recency)\n",
        "    y = (clv_demo['Recency'] > clv_demo['Recency'].median()).astype(int)\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "    # Fit RandomForestClassifier model\n",
        "    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    # Evaluate model\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "id": "1LYgvhsDdkef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "scores = cross_val_score(clf, X, y, cv=5)\n",
        "print(scores, scores.mean())\n",
        "\n",
        "import pandas as pd\n",
        "pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)"
      ],
      "metadata": {
        "id": "vamKMn6NfiJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Secara statistik, model Anda bekerja sempurna pada dataset uji saat ini.\n",
        "\n",
        "Namun, hasil ini terlalu bagus untuk jadi kenyataan — kemungkinan besar ada kebocoran data atau overfitting.\n",
        "\n",
        "Perlu validasi silang waktu atau cross-validation untuk memastikan performa tersebut stabil."
      ],
      "metadata": {
        "id": "srZJDN7NfqiW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lain-lain"
      ],
      "metadata": {
        "id": "VoiIvTNzhDVN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analisis lain-lain masih dalam tahap beta (uji coba). Comments and suggestions are welcome."
      ],
      "metadata": {
        "id": "2RmMiQ5KXINs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Origin–Destination (OD) Network Analysis"
      ],
      "metadata": {
        "id": "x7jtgYvrg9pk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Identify the most frequent passenger movement patterns between stops (e.g., home–work corridors) to reveal passenger flow hotspots for route optimization or congestion management."
      ],
      "metadata": {
        "id": "EOIT9QlQhH_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "\n",
        "edges = (df1.groupby(['tapInStopsName', 'tapOutStopsName'])\n",
        "           .size().reset_index(name='weight'))\n",
        "\n",
        "G = nx.from_pandas_edgelist(edges, 'tapInStopsName', 'tapOutStopsName', edge_attr='weight', create_using=nx.DiGraph())\n",
        "\n",
        "# Centrality metrics\n",
        "centrality = nx.degree_centrality(G)\n",
        "betweenness = nx.betweenness_centrality(G, weight='weight')\n",
        "\n",
        "# Top “hub” stops\n",
        "sorted(centrality.items(), key=lambda x: x[1], reverse=True)[:10]"
      ],
      "metadata": {
        "id": "pzSH_fXsf5cR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Customer Mobility Typology (Behavioral Clustering)"
      ],
      "metadata": {
        "id": "kd2aW7zjhR4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_trips = (df1.groupby('payCardName')\n",
        "                 .agg(trips=('transID','count'),\n",
        "                      unique_stops=('tapInStopsName','nunique'),\n",
        "                      mean_trip_hours=('tripDuration_hours','mean'),\n",
        "                      weekdays=('tapDay','nunique'))\n",
        "                 .reset_index())\n",
        "display(user_trips.head()) # Add display to show the result"
      ],
      "metadata": {
        "id": "OSA3bw6agKNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bank/Payment Analytics"
      ],
      "metadata": {
        "id": "zTMkwVN-hUv2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bank_stats = (df1.groupby('payCardBank')\n",
        "                .agg(avg_spent=('payAmount','mean'),\n",
        "                     trips=('transID','count'),\n",
        "                     unique_users=('payCardName','nunique'))\n",
        "                .reset_index())\n",
        "bank_stats.plot(kind='bar', x='payCardBank', y='avg_spent')"
      ],
      "metadata": {
        "id": "_g0bY1PZgL_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spatial Density Mapping"
      ],
      "metadata": {
        "id": "8TXp5auHhab7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import folium\n",
        "from folium.plugins import HeatMap\n",
        "\n",
        "heat_data = df1[['tapInStopsLat','tapInStopsLon']].values.tolist()\n",
        "m = folium.Map(location=[-6.1751,106.8272], zoom_start=11)\n",
        "HeatMap(heat_data, radius=10).add_to(m)\n",
        "m"
      ],
      "metadata": {
        "id": "sqHdMWPygOte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Origin–Destination Network + HeatMap overlay"
      ],
      "metadata": {
        "id": "8RfAzFK9huOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import folium\n",
        "from folium.plugins import HeatMap\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Defensive copy\n",
        "od_df = df1.copy()\n",
        "\n",
        "# Ensure stop name columns exist\n",
        "required_cols = ['tapInStopsName','tapOutStopsName',\n",
        "                 'tapInStopsLat','tapInStopsLon','tapOutStopsLat','tapOutStopsLon']\n",
        "missing_cols = [c for c in required_cols if c not in od_df.columns]\n",
        "if missing_cols:\n",
        "    raise ValueError(f\"Missing required columns in df1: {missing_cols}\")\n",
        "\n",
        "# 1) Aggregate OD flows (weight = number of trips)\n",
        "od_agg = (od_df\n",
        "          .groupby(['tapInStopsName','tapOutStopsName',\n",
        "                    'tapInStopsLat','tapInStopsLon','tapOutStopsLat','tapOutStopsLon'])\n",
        "          .size()\n",
        "          .reset_index(name='weight')\n",
        "         )\n",
        "\n",
        "# Drop self-loops if you want (optional)\n",
        "od_agg = od_agg[od_agg['tapInStopsName'] != od_agg['tapOutStopsName']].reset_index(drop=True)\n",
        "\n",
        "# 2) Build directed NetworkX graph\n",
        "G = nx.DiGraph()\n",
        "for _, row in od_agg.iterrows():\n",
        "    src = row['tapInStopsName']\n",
        "    dst = row['tapOutStopsName']\n",
        "    w = int(row['weight'])\n",
        "    # add nodes with coords as attributes (keep first encountered coords)\n",
        "    if src not in G.nodes:\n",
        "        G.add_node(src, lat=row['tapInStopsLat'], lon=row['tapInStopsLon'])\n",
        "    if dst not in G.nodes:\n",
        "        G.add_node(dst, lat=row['tapOutStopsLat'], lon=row['tapOutStopsLon'])\n",
        "    # if edge already exists, sum weights (shouldn't if grouped)\n",
        "    if G.has_edge(src, dst):\n",
        "        G[src][dst]['weight'] += w\n",
        "    else:\n",
        "        G.add_edge(src, dst, weight=w)\n",
        "\n",
        "print(f\"Nodes: {G.number_of_nodes()}, Edges: {G.number_of_edges()}\")\n",
        "\n",
        "# 3) Compute simple centrality measures (degree and betweenness)\n",
        "degree_cen = dict(G.degree(weight='weight'))       # weighted degree (in+out)\n",
        "in_deg = dict(G.in_degree(weight='weight'))\n",
        "out_deg = dict(G.out_degree(weight='weight'))\n",
        "betw = nx.betweenness_centrality(G, weight='weight', normalized=True)\n",
        "\n",
        "# Create dataframe for nodes ranking\n",
        "nodes_df = pd.DataFrame([\n",
        "    {'stop': n,\n",
        "     'lat': G.nodes[n].get('lat'),\n",
        "     'lon': G.nodes[n].get('lon'),\n",
        "     'degree': degree_cen.get(n,0),\n",
        "     'in_degree': in_deg.get(n,0),\n",
        "     'out_degree': out_deg.get(n,0),\n",
        "     'betweenness': betw.get(n,0)}\n",
        "    for n in G.nodes()\n",
        "])\n",
        "nodes_df = nodes_df.sort_values('degree', ascending=False).reset_index(drop=True)\n",
        "nodes_df.head(10)\n",
        "\n",
        "# 4) Extract top K edges by weight for mapping\n",
        "top_k = 30   # adjustable: how many top OD links to draw\n",
        "edges_df = pd.DataFrame([ (u, v, d['weight']) for u,v,d in G.edges(data=True) ],\n",
        "                        columns=['source','target','weight'])\n",
        "edges_top = edges_df.sort_values('weight', ascending=False).head(top_k).reset_index(drop=True)\n",
        "edges_top['source_lat'] = edges_top['source'].map(nodes_df.set_index('stop')['lat'])\n",
        "edges_top['source_lon'] = edges_top['source'].map(nodes_df.set_index('stop')['lon'])\n",
        "edges_top['target_lat'] = edges_top['target'].map(nodes_df.set_index('stop')['lat'])\n",
        "edges_top['target_lon'] = edges_top['target'].map(nodes_df.set_index('stop')['lon'])\n",
        "edges_top = edges_top.dropna(subset=['source_lat','source_lon','target_lat','target_lon']).reset_index(drop=True)\n",
        "\n",
        "# 5) Build Folium map centered on Jakarta (or centroid of stops)\n",
        "if nodes_df[['lat','lon']].dropna().shape[0] > 0:\n",
        "    center = [nodes_df['lat'].median(), nodes_df['lon'].median()]\n",
        "else:\n",
        "    center = [-6.1751,106.8272]  # fallback\n",
        "m = folium.Map(location=center, zoom_start=11, tiles='Cartodb Positron')\n",
        "\n",
        "# 6) Add HeatMap for tap-in locations (all tap-in events, weighted by one)\n",
        "heat_data = od_df[['tapInStopsLat','tapInStopsLon']].dropna().values.tolist()\n",
        "HeatMap(heat_data, radius=8, blur=12, max_zoom=13).add_to(m)\n",
        "\n",
        "# 7) Add top OD polylines with thickness proportional to weight\n",
        "max_w = edges_top['weight'].max() if not edges_top.empty else 1\n",
        "min_w = edges_top['weight'].min() if not edges_top.empty else 1\n",
        "for _, r in edges_top.iterrows():\n",
        "    line = [(r['source_lat'], r['source_lon']), (r['target_lat'], r['target_lon'])]\n",
        "    # line thickness scaled (tweak multiplier as needed)\n",
        "    weight = 1 + 8 * ((r['weight'] - min_w) / (max_w - min_w + 1e-9))\n",
        "    folium.PolyLine(locations=line,\n",
        "                    weight=weight,\n",
        "                    color='crimson',\n",
        "                    opacity=0.7,\n",
        "                    tooltip=f\"{r['source']} → {r['target']}: {r['weight']} trips\"\n",
        "                   ).add_to(m)\n",
        "    # add small circle markers for endpoints for clarity\n",
        "    folium.CircleMarker(location=(r['source_lat'], r['source_lon']),\n",
        "                        radius=2,\n",
        "                        color='blue',\n",
        "                        fill=True,\n",
        "                        fill_opacity=0.6).add_to(m)\n",
        "    folium.CircleMarker(location=(r['target_lat'], r['target_lon']),\n",
        "                        radius=2,\n",
        "                        color='green',\n",
        "                        fill=True,\n",
        "                        fill_opacity=0.6).add_to(m)\n",
        "\n",
        "# 8) Add a layer with top nodes labeled (optional)\n",
        "for _, nr in nodes_df.head(20).iterrows():\n",
        "    folium.CircleMarker([nr['lat'], nr['lon']],\n",
        "                        radius=4 + (nr['degree'] / (nodes_df['degree'].max()+1))*6,\n",
        "                        color='darkblue',\n",
        "                        fill=True, fill_opacity=0.8,\n",
        "                        popup=f\"{nr['stop']} (deg={int(nr['degree'])}, bw={nr['betweenness']:.3f})\").add_to(m)\n",
        "\n",
        "# 9) Save or display map\n",
        "# In Colab this will render; use m.save('od_network_map.html') to export\n",
        "m"
      ],
      "metadata": {
        "id": "kuEk1CYchqQ3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}